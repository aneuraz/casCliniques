{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee6db6e-31a9-4122-b55f-aeccec6418a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from brat2conll import Brat2Conll, split_train_test\n",
    "import glob\n",
    "import os \n",
    "import shutil\n",
    "import pandas as pd\n",
    "import random \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3401e6-f02a-409c-9b02-d7b536575fc9",
   "metadata": {},
   "source": [
    "Recopie les fichiers text manquants dans le dossier annotated_data à partir du dossier raw_text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bece1c0-827b-4d6d-868d-8ddff5b67c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_list = glob.glob('annotated_data/*.ann')\n",
    "txt_test = [x.replace('.ann','.txt') for x in ann_list]\n",
    "for txt_target in txt_test: \n",
    "    if not os.path.isfile(txt_target):\n",
    "        txt_source = txt_target.replace('annotated_data', 'raw_text')\n",
    "        print(txt_source)\n",
    "        shutil.copyfile(txt_source, txt_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa5f00c-b48d-4c43-92b7-0197d00f3d4b",
   "metadata": {},
   "source": [
    "Converti les fichiers .ann en dataframe aggrégé par phrase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65131832-978e-43db-9db9-78ba0e0a906e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ped_008</td>\n",
       "      <td>0</td>\n",
       "      <td>[Histoire, clinique, L’, interrogatoire, est, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ped_008</td>\n",
       "      <td>1</td>\n",
       "      <td>[Nous, attribuerons, par, la, suite, cette, at...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ped_008</td>\n",
       "      <td>2</td>\n",
       "      <td>[L’, examen, clinique, initial, trouve, un, ét...</td>\n",
       "      <td>[O, O, O, O, O, O, disorder, disorder, disorde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ped_008</td>\n",
       "      <td>3</td>\n",
       "      <td>[L’, examen, abdominal, met, en, évidence, une...</td>\n",
       "      <td>[O, procedure, procedure, O, O, O, O, disorder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ped_008</td>\n",
       "      <td>4</td>\n",
       "      <td>[Le, toucher, vaginal, est, sans, anomalie, no...</td>\n",
       "      <td>[O, disorder, disorder, disorder, disorder, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24313</th>\n",
       "      <td>unil_007</td>\n",
       "      <td>26</td>\n",
       "      <td>[Tonus, normal, .]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24314</th>\n",
       "      <td>unil_007</td>\n",
       "      <td>27</td>\n",
       "      <td>[ROT, faibles, aux, 4, membres, ,, avec, RCP, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24315</th>\n",
       "      <td>unil_007</td>\n",
       "      <td>28</td>\n",
       "      <td>[Force, et, sensibilité, tacto, -, algique, co...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24316</th>\n",
       "      <td>unil_007</td>\n",
       "      <td>29</td>\n",
       "      <td>[Pas, de, troubles, grossiers, de, la, coordin...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24317</th>\n",
       "      <td>unil_007</td>\n",
       "      <td>30</td>\n",
       "      <td>[\\n \\n \\n\\n\\n\\n\\n\\n\\n, Etape, suivante, :, cho...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24318 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         doc_id  sent_id                                              token  \\\n",
       "0       ped_008        0  [Histoire, clinique, L’, interrogatoire, est, ...   \n",
       "1       ped_008        1  [Nous, attribuerons, par, la, suite, cette, at...   \n",
       "2       ped_008        2  [L’, examen, clinique, initial, trouve, un, ét...   \n",
       "3       ped_008        3  [L’, examen, abdominal, met, en, évidence, une...   \n",
       "4       ped_008        4  [Le, toucher, vaginal, est, sans, anomalie, no...   \n",
       "...         ...      ...                                                ...   \n",
       "24313  unil_007       26                                 [Tonus, normal, .]   \n",
       "24314  unil_007       27  [ROT, faibles, aux, 4, membres, ,, avec, RCP, ...   \n",
       "24315  unil_007       28  [Force, et, sensibilité, tacto, -, algique, co...   \n",
       "24316  unil_007       29  [Pas, de, troubles, grossiers, de, la, coordin...   \n",
       "24317  unil_007       30  [\\n \\n \\n\\n\\n\\n\\n\\n\\n, Etape, suivante, :, cho...   \n",
       "\n",
       "                                                   label  \n",
       "0      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2      [O, O, O, O, O, O, disorder, disorder, disorde...  \n",
       "3      [O, procedure, procedure, O, O, O, O, disorder...  \n",
       "4      [O, disorder, disorder, disorder, disorder, di...  \n",
       "...                                                  ...  \n",
       "24313                                          [O, O, O]  \n",
       "24314                  [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "24315                           [O, O, O, O, O, O, O, O]  \n",
       "24316               [O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "24317               [O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "\n",
       "[24318 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter = Brat2Conll()\n",
    "\n",
    "df = pd.concat(converter.convert(\"annotated_data\"))\n",
    "df_agg = df.groupby(['doc_id','sent_id'])\\\n",
    "            .agg({'token':lambda x: [tok.text for tok in list(x)],\n",
    "                  'label': lambda x : list(x)})\\\n",
    "            .reset_index()\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac359c63-239a-4718-adac-80d59ebe20e1",
   "metadata": {},
   "source": [
    "Supprime les phrases avec des labels \"irrelevant\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb37de1d-bc01-4d18-b42d-631c83bf8092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_filtered = df_agg[df_agg['label'].apply(lambda x: 'irrelevant' not in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0866aa-0f66-4610-a92b-d553e890790e",
   "metadata": {},
   "source": [
    "Transforme le dataframe en list de dictionnaires compatible avec le format json et effectue le split en train/test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d605b01-194e-43f6-a83d-86eddd1edcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = split_train_test(df_agg_filtered, seed = 23, train_frac = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bba717-e2b5-4830-a6cb-f8675e194f1a",
   "metadata": {},
   "source": [
    "Enregistre les train et test sets dans des fichiers json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02a532d7-1075-4d64-85f5-6a62d26d831a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/trainset.json', 'w', encoding='utf8') as f: \n",
    "    json.dump(trainset, f)\n",
    "with open('datasets/testset.json', 'w', encoding='utf8') as f: \n",
    "    json.dump(testset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
